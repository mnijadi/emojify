{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojify\n",
    "\n",
    "Let's try to build a model that, given an short text sentence as input, tries to find an adequate emoji.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base packages\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emojifier-V1\n",
    "\n",
    "This first version of the Emojifer is going to be our baseline model, the model performs the following steps:\n",
    "- takes as input a sentence\n",
    "- finds the vector representation for each word in the sentence, and average these vetors into one\n",
    "- feeds this average vector to a dense layer with a softmax activation for classification\n",
    "\n",
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('data/tesss.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find the largest sentence for 0 padding all others to have the same size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=lambda x: len(x.split())).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a sample from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never talk to me again 😞\n",
      "I am proud of your achievements 😄\n",
      "It is the worst day in my life 😞\n",
      "Miss you so much ❤️\n",
      "food is life 🍴\n",
      "I love you mum ❤️\n",
      "Stop saying bullshit 😞\n",
      "congratulations on your acceptance 😄\n",
      "The assignment is too long  😞\n",
      "I want to go play ⚾\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(X_train[idx], label_to_emoji(Y_train[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to one hot encode the target vector to be appropriate with the 'softmax' activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 'I missed you' has label index 0, which is emoji ❤️\n",
      "Label index 0 in one-hot encoding format is [1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "idx = 50\n",
    "print(f\"Sentence '{X_train[idx]}' has label index {Y_train[idx]}, which is emoji {label_to_emoji(Y_train[idx])}\", )\n",
    "print(f\"Label index {Y_train[idx]} in one-hot encoding format is {Y_oh_train[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of cucumber in the vocabulary is 113317\n",
      "the 289846th word in the vocabulary is potatos\n"
     ]
    }
   ],
   "source": [
    "word = \"cucumber\"\n",
    "idx = 289846\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(idx) + \"th word in the vocabulary is\", index_to_word[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function that converts the input sentence to the average vector that is going to feed into the softmax dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    \n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "    \n",
    "    words = sentence.lower().split()\n",
    "\n",
    "    avg = np.zeros(word_to_vec_map[any_word].shape)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for w in words:\n",
    "        if w in word_to_vec_map:\n",
    "            avg += word_to_vec_map[w]\n",
    "            count +=1\n",
    "          \n",
    "    if count > 0:\n",
    "        avg = avg / count\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = \n",
      " [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
      " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
      "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
      "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
      "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
      "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
      " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
      " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
      "  0.1445417   0.09808667]\n"
     ]
    }
   ],
   "source": [
    "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
    "print(\"avg = \\n\", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "\n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "    \n",
    "    m = Y.shape[0]                             # number of training examples\n",
    "    n_y = len(np.unique(Y))                    # number of classes  \n",
    "    n_h = word_to_vec_map[any_word].shape[0]   # dimensions of the GloVe vectors \n",
    "    \n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        \n",
    "        cost = 0\n",
    "        dW = 0\n",
    "        db = 0\n",
    "        \n",
    "        for i in range(m):\n",
    "            \n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "\n",
    "            z = np.dot(W, avg) + b\n",
    "            a = softmax(z)\n",
    "\n",
    "            cost += - np.sum(Y_oh[i] * a)\n",
    "    \n",
    "            dz = a - Y_oh[i]\n",
    "            dW += np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db += dz\n",
    "\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "            \n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map)\n",
    "\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = -47.92582129545767\n",
      "Accuracy: 0.5454545454545454\n",
      "Epoch: 100 --- cost = -124.63320353765374\n",
      "Accuracy: 0.9318181818181818\n",
      "Epoch: 200 --- cost = -131.30328140693368\n",
      "Accuracy: 1.0\n",
      "Epoch: 300 --- cost = -131.69623486452446\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Accuracy: 1.0\n",
      "Test set:\n",
      "Accuracy: 0.9107142857142857\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print('Test set:')\n",
    "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does a very good job compared to random guessing that would have an accuracy of only 20%.  \n",
    "Note how the performance is good with only a few training examples, this is because we are using pre-trained word embeddings where words carry significant meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n",
      "\n",
      "i treasure you ❤️\n",
      "i love you ❤️\n",
      "funny lol 😄\n",
      "lets play with a ball ⚾\n",
      "food is ready 🍴\n",
      "today is not good 😄\n"
     ]
    }
   ],
   "source": [
    "X_my_sentences = np.array([\"i treasure you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"today is not good\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the model doesn't account for word order, which is completely normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ❤️    ⚾    😄    😞   🍴\n",
      "Predicted  0.0  1.0  2.0  3.0  4.0  All\n",
      "Actual                                 \n",
      "0            6    0    0    1    0    7\n",
      "1            0    8    0    0    0    8\n",
      "2            1    0   17    0    0   18\n",
      "3            1    0    2   13    0   16\n",
      "4            0    0    0    0    7    7\n",
      "All          8    8   19   14    7   56\n"
     ]
    }
   ],
   "source": [
    "print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))\n",
    "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that emojis related to emotions have a harder time to be mislabeled than other emojis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emojifier-V2\n",
    "\n",
    "This new model is supposed to accout for more complexe relationships, such as word orders.  \n",
    "On top of the a word embedding layer, we'll use two layers of LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow\n",
    "np.random.seed(0)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new model takes as input word indices and not the sentence as it is, we need a function to convert sentence to word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        sentence_words = X[i].lower().split()\n",
    "        \n",
    "        j = 0\n",
    "        \n",
    "        for w in sentence_words:\n",
    "            if w in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "                j += 1\n",
    "            \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our function words as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['funny lol' 'lets play baseball' 'food is ready for you']\n",
      "X1_indices =\n",
      " [[155345. 225122.      0.      0.      0.]\n",
      " [220930. 286375.  69714.      0.      0.]\n",
      " [151204. 192973. 302254. 151349. 394475.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([\"funny lol\", \"lets play baseball\", \"food is ready for you\"])\n",
    "X1_indices = sentences_to_indices(X1, word_to_index, max_len=5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\\n\", X1_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create our custom Keras embedding layer that uses the GloVe words representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    any_word = list(word_to_vec_map.keys())[0]\n",
    "    emb_dim = word_to_vec_map[any_word].shape[0]\n",
    "      \n",
    "    emb_matrix = np.zeros((vocab_size, emb_dim))\n",
    "    \n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size, emb_dim)\n",
    "\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights[0][1][1] = 0.39031\n",
      "Input_dim 400001\n",
      "Output_dim 50\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "print(\"weights[0][1][1] =\", embedding_layer.get_weights()[0][1][1])\n",
    "print(\"Input_dim\", embedding_layer.input_dim)\n",
    "print(\"Output_dim\",embedding_layer.output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the pieces required to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
    "\n",
    "    sentence_indices = Input(shape=input_shape, dtype='int32')\n",
    "    \n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(5)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    model = Model(sentence_indices, X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 20,223,927\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before fitting the model, we need to preprocess our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 2s 373ms/step - loss: 1.5936 - accuracy: 0.2348\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 2s 361ms/step - loss: 1.4850 - accuracy: 0.3333\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 2s 346ms/step - loss: 1.4442 - accuracy: 0.4015\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 2s 346ms/step - loss: 1.3602 - accuracy: 0.4773\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 2s 343ms/step - loss: 1.2044 - accuracy: 0.5985\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 2s 367ms/step - loss: 1.0520 - accuracy: 0.6818\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 2s 345ms/step - loss: 0.8450 - accuracy: 0.7273\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 2s 339ms/step - loss: 0.6870 - accuracy: 0.7576\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 2s 343ms/step - loss: 0.5656 - accuracy: 0.7803\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 2s 345ms/step - loss: 0.4747 - accuracy: 0.8561\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 2s 361ms/step - loss: 0.4460 - accuracy: 0.8258\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 2s 354ms/step - loss: 0.3523 - accuracy: 0.8788\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 2s 343ms/step - loss: 0.4099 - accuracy: 0.8636\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 2s 340ms/step - loss: 0.3613 - accuracy: 0.8561\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 2s 357ms/step - loss: 0.2912 - accuracy: 0.9015\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 2s 357ms/step - loss: 0.2028 - accuracy: 0.9470\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 2s 343ms/step - loss: 0.1694 - accuracy: 0.9545\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 2s 352ms/step - loss: 0.1470 - accuracy: 0.9545\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 2s 356ms/step - loss: 0.1136 - accuracy: 0.9621\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 2s 344ms/step - loss: 0.0741 - accuracy: 0.9773\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 2s 354ms/step - loss: 0.0575 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 2s 356ms/step - loss: 0.0578 - accuracy: 0.9848\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 2s 357ms/step - loss: 0.0489 - accuracy: 0.9848\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 2s 356ms/step - loss: 0.0465 - accuracy: 0.9848\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 2s 353ms/step - loss: 0.0892 - accuracy: 0.9697\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 2s 353ms/step - loss: 0.0664 - accuracy: 0.9773\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 2s 360ms/step - loss: 0.0583 - accuracy: 0.9848\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 2s 355ms/step - loss: 0.0764 - accuracy: 0.9621\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 2s 356ms/step - loss: 0.0828 - accuracy: 0.9848\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 2s 346ms/step - loss: 0.0531 - accuracy: 0.9848\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 2s 340ms/step - loss: 0.0340 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 2s 354ms/step - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 2s 344ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 2s 340ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 2s 352ms/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 2s 342ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 2s 340ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 2s 362ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 2s 344ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 2s 353ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 2s 341ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 2s 344ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 2s 343ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 2s 345ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 2s 346ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 2s 351ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 2s 343ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 2s 340ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 2s 342ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 2s 346ms/step - loss: 0.0017 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7feecd2bcf50>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 1.1109 - accuracy: 0.8036\n",
      "\n",
      "Test accuracy =  0.8035714030265808\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see where our model got it wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected emoji:😄 prediction: he got a very nice raise\t❤️\n",
      "Expected emoji:😄 prediction: she got me a nice present\t❤️\n",
      "Expected emoji:😞 prediction: work is hard\t😄\n",
      "Expected emoji:😞 prediction: This girl is messing with me\t❤️\n",
      "Expected emoji:😞 prediction: work is horrible\t😄\n",
      "Expected emoji:🍴 prediction: any suggestions for dinner\t😄\n",
      "Expected emoji:😄 prediction: you brighten my day\t❤️\n",
      "Expected emoji:😞 prediction: she is a bully\t❤️\n",
      "Expected emoji:😞 prediction: My life is so boring\t❤️\n",
      "Expected emoji:😄 prediction: will you be my valentine\t❤️\n",
      "Expected emoji:😞 prediction: go away\t⚾\n"
     ]
    }
   ],
   "source": [
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps, the reason we got these misclassifications is due to the fact that our pretrained word embeddings weren't large enough to cover the meaning of all the words we have in our sentences.  \n",
    "Now, we can try the model on a custom input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are we eating for dinner ? 🍴\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['What are we eating for dinner ?'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new version of the model does account for word order and performs better than the baseline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
